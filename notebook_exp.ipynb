{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joanvelja/miniconda3/envs/RLHF/lib/python3.9/site-packages/tqdm-4.66.2-py3.9.egg/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from stealing_gridworld import StealingGridworld\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"environment\": {\n",
    "        \"grid_size\": 5,\n",
    "        \"horizon\": 30,\n",
    "        \"reward_for_depositing\": 100,\n",
    "        \"reward_for_picking_up\": 1,\n",
    "        \"reward_for_stealing\": -200,\n",
    "    },\n",
    "    \"reward_model\": {\n",
    "        \"type\": \"NonImageCnnRewardNet\",\n",
    "        \"hid_channels\": [32, 32],\n",
    "        \"kernel_size\": 3,\n",
    "    },\n",
    "    \"seed\": 0,\n",
    "    \"dataset_max_size\": 2000,\n",
    "    # If fragment_length is None, then the whole trajectory is used as a single fragment.\n",
    "    \"fragment_length\": 12,\n",
    "    \"transition_oversampling\": 10,\n",
    "    \"initial_epoch_multiplier\": 1.0,\n",
    "    \"feedback\": {\n",
    "        \"type\": \"preference\",\n",
    "    },\n",
    "    \"trajectory_generator\": {\n",
    "        \"epsilon\": 0.1,\n",
    "    },\n",
    "    \"visibility\": {\n",
    "        \"visibility\": \"partial\",\n",
    "        # Available visibility mask keys:\n",
    "        # \"full\": All of the grid is visible. Not actually used, but should be set for easier comparison.\n",
    "        # \"(n-1)x(n-1)\": All but the outermost ring of the grid is visible.\n",
    "        #\"visibility_mask_key\": \"(n-1)x(n-1)\",\n",
    "        \"visibility_mask_key\": \"camera\",\n",
    "    },\n",
    "    \"reward_trainer\": {\n",
    "        \"num_epochs\": 5,\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = StealingGridworld(**config['environment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = StealingGridworld(\n",
    "    grid_size=config[\"environment\"][\"grid_size\"],\n",
    "    horizon=config[\"environment\"][\"horizon\"],\n",
    "    reward_for_depositing=config[\"environment\"][\"reward_for_depositing\"],\n",
    "    reward_for_picking_up=config[\"environment\"][\"reward_for_picking_up\"],\n",
    "    reward_for_stealing=config[\"environment\"][\"reward_for_stealing\"],\n",
    "    seed = config[\"seed\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs5_nfp2_nop1_rfd100_rfp1_rfs-200'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.params_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stealing_gridworld import DynamicGridVisibility_OJ\n",
    "\n",
    "camera = DynamicGridVisibility_OJ(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+\n",
      "| # | # |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "| # | # |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "+---+---+---+---+---+\n",
      "|   | # | # |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   | # | # |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "+---+---+---+---+---+\n",
      "|   |   | # | # |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   | # | # |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "+---+---+---+---+---+\n",
      "|   |   |   | # | # |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   | # | # |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   | # | # |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   | # | # |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   | # | # |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   | # | # |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   | # | # |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   | # | # |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "| # | # |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "| # | # |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "| # | # |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "| # | # |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   | # | # |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   | # | # |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   | # | # |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   | # | # |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   | # | # |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   | # | # |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   | # | # |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   | # | # |\n",
      "+---+---+---+---+---+\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   | # | # |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   | # | # |   |\n",
      "+---+---+---+---+---+\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   | # | # |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   | # | # |   |   |\n",
      "+---+---+---+---+---+\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "| # | # |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "| # | # |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "+---+---+---+---+---+\n",
      "| # | # |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "| # | # |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "+---+---+---+---+---+\n",
      "|   | # | # |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   | # | # |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "+---+---+---+---+---+\n",
      "|   |   | # | # |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   | # | # |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "+---+---+---+---+---+\n",
      "|   |   |   | # | # |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   | # | # |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   | # | # |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   | # | # |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   | # | # |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   | # | # |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   | # | # |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   | # | # |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "| # | # |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "| # | # |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "| # | # |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "| # | # |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   | # | # |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   | # | # |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   | # | # |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   | # | # |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   | # | # |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   | # | # |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   | # | # |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   | # | # |\n",
      "+---+---+---+---+---+\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   | # | # |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   | # | # |   |\n",
      "+---+---+---+---+---+\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   | # | # |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   | # | # |   |   |\n",
      "+---+---+---+---+---+\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "| # | # |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "| # | # |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "+---+---+---+---+---+\n",
      "| # | # |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "| # | # |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "+---+---+---+---+---+\n",
      "|   | # | # |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   | # | # |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "+---+---+---+---+---+\n",
      "|   |   | # | # |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   | # | # |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "+---+---+---+---+---+\n",
      "|   |   |   | # | # |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   | # | # |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   | # | # |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   | # | # |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   | # | # |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   | # | # |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   | # | # |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   | # | # |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "| # | # |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "| # | # |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "| # | # |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "| # | # |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n"
     ]
    }
   ],
   "source": [
    "masks = camera.update_visibility(t=40)\n",
    "\n",
    "for i in masks:\n",
    "    camera.render_mask(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(87237) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(87238) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch as th\n",
    "from imitation.util import logger as imit_logger\n",
    "\n",
    "import wandb\n",
    "import os\n",
    "import abc\n",
    "from evaluate_reward_model import full_visibility_evaluator_factory, partial_visibility_evaluator_factory, camera_visibility_evaluator_factory\n",
    "from imitation_modules import (\n",
    "    BasicScalarFeedbackRewardTrainer,\n",
    "    DeterministicMDPTrajGenerator,\n",
    "    MSERewardLoss,\n",
    "    NoisyObservationGathererWrapper,\n",
    "    NonImageCnnRewardNet,\n",
    "    RandomSingleFragmenter,\n",
    "    ScalarFeedbackModel,\n",
    "    ScalarRewardLearner,\n",
    "    SyntheticScalarFeedbackGatherer,\n",
    ")\n",
    "from imitation_modules import (\n",
    "    PreferenceComparisons,\n",
    "    PreferenceModel,\n",
    "    BasicRewardTrainer,\n",
    "    CrossEntropyRewardLoss,\n",
    "    SyntheticGatherer,\n",
    "    RandomFragmenter,\n",
    "    PreferenceComparisonNoisyObservationGathererWrapper,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from stealing_gridworld import PartialGridVisibility, DynamicGridVisibility\n",
    "\n",
    "# class ObservationFunction(abc.ABC):\n",
    "#     \"\"\"Abstract class for functions that take an observation and return a new observation.\"\"\"\n",
    "\n",
    "#     @abc.abstractmethod\n",
    "#     def __call__(self, fragment):\n",
    "#         \"\"\"Returns a new fragment with observations, actions, and rewards filtered through an observation function.\n",
    "\n",
    "#         Args:\n",
    "#             fragment: a TrajectoryWithRew object.\n",
    "\n",
    "#         Returns:\n",
    "#             A new TrajectoryWithRew object with the same infos and terminal flag, but with the observations, actions,\n",
    "#             and rewards filtered through the observation function.\n",
    "#         \"\"\"\n",
    "\n",
    "# class DynamicGridVisibility(ObservationFunction):\n",
    "#     def __init__(self, env: StealingGridworld, pattern=None, feedback=\"scalar\"):\n",
    "#         super().__init__()\n",
    "#         self.env = env\n",
    "#         self.grid_size = env.grid_size\n",
    "#         self.feedback = feedback\n",
    "        \n",
    "#         # Define the pattern of camera movement\n",
    "#         if pattern is None:\n",
    "#             self.pattern = self.default_pattern()\n",
    "#         else:\n",
    "#             self.pattern = pattern\n",
    "#         print(\"Pattern = \", self.pattern)\n",
    "#         self.pattern_index = 0  # Start at the first position in the pattern\n",
    "\n",
    "#         # Build the initial visibility mask\n",
    "#         self.visibility_mask = self.construct_visibility_mask()\n",
    "\n",
    "#     def default_pattern(self):\n",
    "#         # Create a default movement pattern for the camera\n",
    "#         # Example for a 5x5 grid, you may adjust as needed\n",
    "#         half_size = self.grid_size // 2 + self.grid_size % 2\n",
    "#         positions = []\n",
    "#         # for i in range(half_size):\n",
    "#         #     for j in range(half_size):\n",
    "#         #         positions.append((i, j))\n",
    "#         # return positions\n",
    "#         # in a 3x3 grid, the camera moves in a spiral pattern\n",
    "#         # the top leftmost corner is (0,0) for both the camera and the grid\n",
    "#         # thus, the camera's top leftmost corner's pattern is going to be:\n",
    "#         # (0,0) -> (0,1) -> (1,1) -> (1,0) \n",
    "\n",
    "#         # HARDCODED, TODO find a way to generalize this\n",
    "#         if self.grid_size == 3:\n",
    "#             positions = [(0,0), (0,1), (1,1), (1,0)]\n",
    "#         elif self.grid_size == 5:\n",
    "#             positions = [(0,0), (0,1), (0,2), (1,2), (2,2), (2,1), (2,0), (1,0)]\n",
    "#         else:\n",
    "#             raise NotImplementedError(\"Default pattern not implemented for grid size other than 3x3 or 5x5\")\n",
    "#         return positions\n",
    "\n",
    "#     def construct_visibility_mask(self):\n",
    "#         # Build a visibility mask based on the current pattern index\n",
    "#         mask = np.zeros((self.grid_size, self.grid_size), dtype=np.bool_)\n",
    "#         left_x, left_y = self.pattern[self.pattern_index]\n",
    "#         camera_size = self.grid_size // 2 + self.grid_size % 2\n",
    "        \n",
    "#         # Calculate bounds of the camera window\n",
    "#         start_x = left_x\n",
    "#         end_x = min(left_x + camera_size, self.grid_size)\n",
    "#         start_y = left_y\n",
    "#         end_y = min(left_y + camera_size, self.grid_size)\n",
    "#         print(\"start_x, end_x, start_y, end_y = \", start_x, end_x, start_y, end_y)\n",
    "#         mask[start_x:end_x, start_y:end_y] = True\n",
    "#         return mask\n",
    "\n",
    "#     def update_visibility(self):\n",
    "#         # Update the visibility mask for the next timestep\n",
    "#         self.pattern_index = (self.pattern_index + 1) % len(self.pattern)\n",
    "#         self.visibility_mask = self.construct_visibility_mask()\n",
    "\n",
    "#     def __call__(self, fragments):\n",
    "#         # Apply the current visibility mask to the fragments\n",
    "#         self.update_visibility()  # Move the camera to the next position\n",
    "#         return super().__call__(fragments)  # Call the base method to apply the mask\n",
    "\n",
    "#     def __repr__(self):\n",
    "#         return f\"DynamicGridVisibility(\\n    grid_size={self.grid_size},\\n    visibility_mask=\\n{self.visibility_mask},\\n    feedback={self.feedback}\\n)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################\n",
    "##################################################### Run params ######################################################\n",
    "#######################################################################################################################\n",
    "\n",
    "\n",
    "GPU_NUMBER = 0\n",
    "N_ITER = 40\n",
    "N_COMPARISONS = 2000\n",
    "TESTING = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward net on mps.\n",
      "Debug new observation function:  DynamicGridVisibility(pattern=[(0, 0), (0, 1), (0, 2), (1, 2), (2, 2), (2, 1), (2, 0), (1, 0)], feedback=preference)\n"
     ]
    }
   ],
   "source": [
    "reward_net = NonImageCnnRewardNet(\n",
    "    env.observation_space,\n",
    "    env.action_space,\n",
    "    hid_channels=config[\"reward_model\"][\"hid_channels\"],\n",
    "    kernel_size=config[\"reward_model\"][\"kernel_size\"],\n",
    ")\n",
    "\n",
    "rng = np.random.default_rng(config[\"seed\"])\n",
    "\n",
    "if GPU_NUMBER is not None:\n",
    "    device = th.device(f\"cuda:{GPU_NUMBER}\" if th.cuda.is_available() else \"mps\" if th.backends.mps.is_available() else 'cpu')\n",
    "    reward_net.to(device)\n",
    "    print(f\"Reward net on {device}.\")\n",
    "\n",
    "if config[\"feedback\"][\"type\"] == 'scalar':\n",
    "    fragmenter = RandomSingleFragmenter(rng=rng)\n",
    "    gatherer = SyntheticScalarFeedbackGatherer(rng=rng)\n",
    "else:\n",
    "    fragmenter = RandomFragmenter(rng=rng)\n",
    "    gatherer = SyntheticGatherer(rng=rng)\n",
    "\n",
    "if config[\"visibility\"][\"visibility\"] == \"partial\":\n",
    "    # visibility_mask = construct_visibility_mask(\n",
    "    #     config[\"environment\"][\"grid_size\"],\n",
    "    #     config[\"visibility\"][\"visibility_mask_key\"],\n",
    "    # )\n",
    "    if config[\"visibility\"][\"visibility_mask_key\"] == \"(n-1)x(n-1)\":\n",
    "        observation_function = PartialGridVisibility(env, mask_key = config[\"visibility\"][\"visibility_mask_key\"], feedback=config[\"feedback\"][\"type\"])\n",
    "        print(\"Debug new observation function: \", observation_function)\n",
    "        policy_evaluator = partial_visibility_evaluator_factory(observation_function.visibility_mask)\n",
    "    elif config[\"visibility\"][\"visibility_mask_key\"] == \"camera\":\n",
    "        observation_function = DynamicGridVisibility(env, feedback=config[\"feedback\"][\"type\"], halt=4)\n",
    "        print(\"Debug new observation function: \", observation_function)\n",
    "        policy_evaluator = camera_visibility_evaluator_factory(observation_function.visibility_mask)\n",
    "\n",
    "    if config[\"feedback\"][\"type\"] == 'scalar':\n",
    "        gatherer = NoisyObservationGathererWrapper(gatherer, observation_function)\n",
    "    elif config[\"feedback\"][\"type\"] == 'preference':\n",
    "        gatherer = PreferenceComparisonNoisyObservationGathererWrapper(gatherer, observation_function)\n",
    "\n",
    "    #policy_evaluator = partial_visibility_evaluator_factory(observation_function.visibility_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_function.reset()\n",
    "\n",
    "masks = observation_function.update_visibility(t = 10, limits=(10,20))\n",
    "\n",
    "for m in masks: \n",
    "    observation_function.render_mask(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(87243) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Enumerating states: 100%|██████████| 25/25 [00:01<00:00, 12.56it/s]\n"
     ]
    }
   ],
   "source": [
    "if config[\"feedback\"][\"type\"] == 'scalar':\n",
    "    feedback_model = ScalarFeedbackModel(model=reward_net)\n",
    "    reward_trainer = BasicScalarFeedbackRewardTrainer(\n",
    "        feedback_model=feedback_model,\n",
    "        loss=MSERewardLoss(),  # Will need to change this for preference learning\n",
    "        rng=rng,\n",
    "        epochs=config[\"reward_trainer\"][\"num_epochs\"],\n",
    "    )\n",
    "\n",
    "else:\n",
    "    feedback_model = PreferenceModel(reward_net)\n",
    "    reward_trainer = BasicRewardTrainer(\n",
    "        preference_model=feedback_model,\n",
    "        loss=CrossEntropyRewardLoss(),\n",
    "        rng=rng,\n",
    "        epochs=config[\"reward_trainer\"][\"num_epochs\"],\n",
    "    )\n",
    "\n",
    "### I think that as long as we are in ValueIteration, this can stay like this?\n",
    "trajectory_generator = DeterministicMDPTrajGenerator(\n",
    "    reward_fn=reward_net,\n",
    "    env=env,\n",
    "    rng=None,  # This doesn't work yet\n",
    "    epsilon=config[\"trajectory_generator\"][\"epsilon\"],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "logger = imit_logger.configure(format_strs=[\"stdout\", \"wandb\"])\n",
    "\n",
    "\n",
    "def save_model_params_anwand_dataset_callback(reward_learner):\n",
    "    data_dir = os.path.join(wandb.run.dir, \"saved_reward_models\")\n",
    "    latest_checkpoint_path = os.path.join(data_dir, \"latest_checkpoint.pt\")\n",
    "    latest_dataset_path = os.path.join(data_dir, \"latest_dataset.pkl\")\n",
    "    checkpoints_dir = os.path.join(data_dir, \"checkpoints\")\n",
    "    checkpoint_iter_path = os.path.join(checkpoints_dir, f\"model_weights_iter{reward_learner._iteration}.pt\")\n",
    "    dataset_iter_path = os.path.join(checkpoints_dir, f\"dataset_iter{reward_learner._iteration}.pkl\")\n",
    "\n",
    "    os.makedirs(checkpoints_dir, exist_ok=True)\n",
    "    th.save(reward_learner.model.state_dict(), latest_checkpoint_path)\n",
    "    th.save(reward_learner.model.state_dict(), checkpoint_iter_path)\n",
    "    reward_learner.dataset.save(latest_dataset_path)\n",
    "    reward_learner.dataset.save(dataset_iter_path)\n",
    "\n",
    "if config[\"feedback\"][\"type\"] == 'scalar':\n",
    "    reward_learner = ScalarRewardLearner(\n",
    "        trajectory_generator=trajectory_generator,\n",
    "        reward_model=reward_net,\n",
    "        num_iterations=N_ITER,\n",
    "        fragmenter=fragmenter,\n",
    "        feedback_gatherer=gatherer,\n",
    "        feedback_queue_size=config[\"dataset_max_size\"],\n",
    "        reward_trainer=reward_trainer,\n",
    "        fragment_length=config[\"fragment_length\"],\n",
    "        transition_oversampling=config[\"transition_oversampling\"],\n",
    "        initial_epoch_multiplier=config[\"initial_epoch_multiplier\"],\n",
    "        policy_evaluator=policy_evaluator,\n",
    "        custom_logger=logger,\n",
    "        #callback=save_model_params_and_dataset_callback,\n",
    "    )\n",
    "\n",
    "else:\n",
    "    reward_learner = PreferenceComparisons(\n",
    "        trajectory_generator=trajectory_generator,\n",
    "        reward_model=reward_net,\n",
    "        num_iterations=N_ITER,\n",
    "        fragmenter=fragmenter,\n",
    "        preference_gatherer=gatherer,\n",
    "        comparison_queue_size=config[\"dataset_max_size\"],\n",
    "        reward_trainer=reward_trainer,\n",
    "        fragment_length=config[\"fragment_length\"],\n",
    "        transition_oversampling=config[\"transition_oversampling\"],\n",
    "        initial_epoch_multiplier=config[\"initial_epoch_multiplier\"],\n",
    "        initial_comparison_frac=0.1,\n",
    "        #query_schedule=\"hyperbolic\",\n",
    "        policy_evaluator=policy_evaluator,\n",
    "        custom_logger=logger,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"feedback\"][\"type\"] == 'scalar':\n",
    "    result = reward_learner.train(\n",
    "        # Just needs to be bigger then N_ITER * HORIZON. Value iteration doesn't really use this.\n",
    "        total_timesteps=10 * N_ITER * wandb.config[\"environment\"][\"horizon\"],\n",
    "        total_queries=N_COMPARISONS,\n",
    "    )\n",
    "\n",
    "else:\n",
    "    result = reward_learner.train(\n",
    "        # Just needs to be bigger then N_ITER * HORIZON. Value iteration doesn't really use this.\n",
    "        total_timesteps=10 * N_ITER * config[\"environment\"][\"horizon\"],\n",
    "        total_comparisons=N_COMPARISONS,\n",
    "        #callback=save_model_params_and_dataset_callback,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = observation_function.update_visibility(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy of code from experiment ipython notebook\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from imitation.util import logger as imit_logger\n",
    "\n",
    "import wandb\n",
    "from evaluate_reward_model import full_visibility_evaluator_factory, partial_visibility_evaluator_factory, camera_visibility_evaluator_factory\n",
    "from imitation_modules import (\n",
    "    BasicScalarFeedbackRewardTrainer,\n",
    "    DeterministicMDPTrajGenerator,\n",
    "    MSERewardLoss,\n",
    "    NoisyObservationGathererWrapper,\n",
    "    NonImageCnnRewardNet,\n",
    "    RandomSingleFragmenter,\n",
    "    ScalarFeedbackModel,\n",
    "    ScalarRewardLearner,\n",
    "    SyntheticScalarFeedbackGatherer,\n",
    ")\n",
    "from imitation_modules import (\n",
    "    PreferenceComparisons,\n",
    "    PreferenceModel,\n",
    "    BasicRewardTrainer,\n",
    "    CrossEntropyRewardLoss,\n",
    "    SyntheticGatherer,\n",
    "    RandomFragmenter,\n",
    "    PreferenceComparisonNoisyObservationGathererWrapper,\n",
    ")\n",
    "\n",
    "from stealing_gridworld import PartialGridVisibility, DynamicGridVisibility, StealingGridworld\n",
    "\n",
    "#######################################################################################################################\n",
    "##################################################### Run params ######################################################\n",
    "#######################################################################################################################\n",
    "\n",
    "\n",
    "GPU_NUMBER = 0\n",
    "N_ITER = 40\n",
    "N_COMPARISONS = 3000\n",
    "TESTING = True\n",
    "\n",
    "\n",
    "#######################################################################################################################\n",
    "##################################################### Expt params #####################################################\n",
    "#######################################################################################################################\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"environment\": {\n",
    "        \"name\": \"StealingGridworld\",\n",
    "        \"grid_size\": 5,\n",
    "        \"horizon\": 30,\n",
    "        \"reward_for_depositing\": 100,\n",
    "        \"reward_for_picking_up\": 1,\n",
    "        \"reward_for_stealing\": -200,\n",
    "    },\n",
    "    \"reward_model\": {\n",
    "        \"type\": \"NonImageCnnRewardNet\",\n",
    "        \"hid_channels\": [32, 32],\n",
    "        \"kernel_size\": 3,\n",
    "    },\n",
    "    \"seed\": 0,\n",
    "    \"dataset_max_size\": 3000,\n",
    "    # If fragment_length is None, then the whole trajectory is used as a single fragment.\n",
    "    \"fragment_length\": 12,\n",
    "    \"transition_oversampling\": 10,\n",
    "    \"initial_epoch_multiplier\": 1.0,\n",
    "    \"feedback\": {\n",
    "        \"type\": \"preference\",\n",
    "    },\n",
    "    \"trajectory_generator\": {\n",
    "        \"epsilon\": 0.1,\n",
    "    },\n",
    "    \"visibility\": {\n",
    "        \"visibility\": \"partial\",\n",
    "        # Available visibility mask keys:\n",
    "        # \"full\": All of the grid is visible. Not actually used, but should be set for easier comparison.\n",
    "        # \"(n-1)x(n-1)\": All but the outermost ring of the grid is visible.\n",
    "        #\"visibility_mask_key\": \"(n-1)x(n-1)\",\n",
    "        \"visibility_mask_key\": \"camera\",\n",
    "    },\n",
    "    \"reward_trainer\": {\n",
    "        \"num_epochs\": 3,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Some validation\n",
    "\n",
    "if config[\"feedback\"][\"type\"] not in (\"scalar\", \"preference\"):\n",
    "    raise NotImplementedError(\"Only scalar and preference feedback are supported at the moment.\")\n",
    "\n",
    "if config[\"visibility\"][\"visibility\"] == \"full\" and config[\"visibility\"][\"visibility_mask_key\"] != \"full\":\n",
    "    raise ValueError(\n",
    "        f'If visibility is \"full\", then visibility mask key must be \"full\".'\n",
    "        f'Instead, it is {config[\"visibility\"][\"visibility_mask_key\"]}.'\n",
    "    )\n",
    "\n",
    "if config[\"visibility\"][\"visibility\"] not in [\"full\", \"partial\"]:\n",
    "    raise ValueError(\n",
    "        f'Unknown visibility {config[\"visibility\"][\"visibility\"]}.' f'Visibility must be \"full\" or \"partial\".'\n",
    "    )\n",
    "\n",
    "if config[\"reward_model\"][\"type\"] != \"NonImageCnnRewardNet\":\n",
    "    raise ValueError(f'Unknown reward model type {config[\"reward_model\"][\"type\"]}.')\n",
    "\n",
    "available_visibility_mask_keys = [\"full\", \"(n-1)x(n-1)\", \"camera\"]\n",
    "if config[\"visibility\"][\"visibility_mask_key\"] not in available_visibility_mask_keys:\n",
    "    raise ValueError(\n",
    "        f'Unknown visibility mask key {config[\"visibility\"][\"visibility_mask_key\"]}.'\n",
    "        f\"Available visibility mask keys are {available_visibility_mask_keys}.\"\n",
    "    )\n",
    "\n",
    "if config[\"fragment_length\"] == None:\n",
    "    config[\"fragment_length\"] = config[\"environment\"][\"horizon\"]\n",
    "    print(\"Fragment length unspecified... setting it to \", config[\"environment\"][\"horizon\"])\n",
    "\n",
    "wandb.login()\n",
    "run = wandb.init(\n",
    "    project=\"assisting-bounded-humans\",\n",
    "    notes=\"Testing the preference comparisons model\",\n",
    "    name=\"Testing camera visibility with preference comparisons\",\n",
    "    tags=[\n",
    "        \"test\",\n",
    "        \"Partial Observability\"\n",
    "    ],\n",
    "    config=config,\n",
    "    mode=\"disabled\" if TESTING else \"online\",\n",
    ")\n",
    "\n",
    "#######################################################################################################################\n",
    "################################################## Create everything ##################################################\n",
    "#######################################################################################################################\n",
    "\n",
    "\n",
    "env = StealingGridworld(\n",
    "    grid_size=wandb.config[\"environment\"][\"grid_size\"],\n",
    "    horizon=wandb.config[\"environment\"][\"horizon\"],\n",
    "    reward_for_depositing=wandb.config[\"environment\"][\"reward_for_depositing\"],\n",
    "    reward_for_picking_up=wandb.config[\"environment\"][\"reward_for_picking_up\"],\n",
    "    reward_for_stealing=wandb.config[\"environment\"][\"reward_for_stealing\"],\n",
    ")\n",
    "\n",
    "\n",
    "reward_net = NonImageCnnRewardNet(\n",
    "    env.observation_space,\n",
    "    env.action_space,\n",
    "    hid_channels=wandb.config[\"reward_model\"][\"hid_channels\"],\n",
    "    kernel_size=wandb.config[\"reward_model\"][\"kernel_size\"],\n",
    ")\n",
    "\n",
    "rng = np.random.default_rng(wandb.config[\"seed\"])\n",
    "\n",
    "if GPU_NUMBER is not None:\n",
    "    device = th.device(f\"cuda:{GPU_NUMBER}\" if th.cuda.is_available() else \"mps\" if th.backends.mps.is_available() else 'cpu')\n",
    "    reward_net.to(device)\n",
    "    print(f\"Reward net on {device}.\")\n",
    "\n",
    "if config[\"feedback\"][\"type\"] == 'scalar':\n",
    "    fragmenter = RandomSingleFragmenter(rng=rng)\n",
    "    gatherer = SyntheticScalarFeedbackGatherer(rng=rng)\n",
    "else:\n",
    "    fragmenter = RandomFragmenter(rng=rng)\n",
    "    gatherer = SyntheticGatherer(rng=rng)\n",
    "\n",
    "if wandb.config[\"visibility\"][\"visibility\"] == \"partial\":\n",
    "    # visibility_mask = construct_visibility_mask(\n",
    "    #     wandb.config[\"environment\"][\"grid_size\"],\n",
    "    #     wandb.config[\"visibility\"][\"visibility_mask_key\"],\n",
    "    # )\n",
    "    if wandb.config[\"visibility\"][\"visibility_mask_key\"] == \"(n-1)x(n-1)\":\n",
    "        observation_function = PartialGridVisibility(env, mask_key = wandb.config[\"visibility\"][\"visibility_mask_key\"], feedback=config[\"feedback\"][\"type\"])\n",
    "        print(\"Debug new observation function: \", observation_function)\n",
    "        policy_evaluator = partial_visibility_evaluator_factory(observation_function.visibility_mask)\n",
    "    elif wandb.config[\"visibility\"][\"visibility_mask_key\"] == \"camera\":\n",
    "        observation_function = DynamicGridVisibility(env, feedback=config[\"feedback\"][\"type\"])\n",
    "        print(\"Debug new observation function: \", observation_function)\n",
    "        policy_evaluator = camera_visibility_evaluator_factory(observation_function.visibility_mask)\n",
    "\n",
    "    if wandb.config[\"feedback\"][\"type\"] == 'scalar':\n",
    "        gatherer = NoisyObservationGathererWrapper(gatherer, observation_function)\n",
    "    elif wandb.config[\"feedback\"][\"type\"] == 'preference':\n",
    "        gatherer = PreferenceComparisonNoisyObservationGathererWrapper(gatherer, observation_function)\n",
    "\n",
    "    #policy_evaluator = partial_visibility_evaluator_factory(observation_function.visibility_mask)\n",
    "\n",
    "elif wandb.config[\"visibility\"][\"visibility\"] == \"full\":\n",
    "    policy_evaluator = full_visibility_evaluator_factory()\n",
    "\n",
    "if config[\"feedback\"][\"type\"] == 'scalar':\n",
    "    feedback_model = ScalarFeedbackModel(model=reward_net)\n",
    "    reward_trainer = BasicScalarFeedbackRewardTrainer(\n",
    "        feedback_model=feedback_model,\n",
    "        loss=MSERewardLoss(),  # Will need to change this for preference learning\n",
    "        rng=rng,\n",
    "        epochs=wandb.config[\"reward_trainer\"][\"num_epochs\"],\n",
    "    )\n",
    "\n",
    "else:\n",
    "    feedback_model = PreferenceModel(reward_net)\n",
    "    reward_trainer = BasicRewardTrainer(\n",
    "        preference_model=feedback_model,\n",
    "        loss=CrossEntropyRewardLoss(),\n",
    "        rng=rng,\n",
    "        epochs=wandb.config[\"reward_trainer\"][\"num_epochs\"],\n",
    "    )\n",
    "\n",
    "### I think that as long as we are in ValueIteration, this can stay like this?\n",
    "trajectory_generator = DeterministicMDPTrajGenerator(\n",
    "    reward_fn=reward_net,\n",
    "    env=env,\n",
    "    rng=None,  # This doesn't work yet\n",
    "    epsilon=wandb.config[\"trajectory_generator\"][\"epsilon\"],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "logger = imit_logger.configure(format_strs=[\"stdout\", \"wandb\"])\n",
    "\n",
    "\n",
    "def save_model_params_and_dataset_callback(reward_learner):\n",
    "    data_dir = os.path.join(wandb.run.dir, \"saved_reward_models\")\n",
    "    latest_checkpoint_path = os.path.join(data_dir, \"latest_checkpoint.pt\")\n",
    "    latest_dataset_path = os.path.join(data_dir, \"latest_dataset.pkl\")\n",
    "    checkpoints_dir = os.path.join(data_dir, \"checkpoints\")\n",
    "    checkpoint_iter_path = os.path.join(checkpoints_dir, f\"model_weights_iter{reward_learner._iteration}.pt\")\n",
    "    dataset_iter_path = os.path.join(checkpoints_dir, f\"dataset_iter{reward_learner._iteration}.pkl\")\n",
    "\n",
    "    os.makedirs(checkpoints_dir, exist_ok=True)\n",
    "    th.save(reward_learner.model.state_dict(), latest_checkpoint_path)\n",
    "    th.save(reward_learner.model.state_dict(), checkpoint_iter_path)\n",
    "    reward_learner.dataset.save(latest_dataset_path)\n",
    "    reward_learner.dataset.save(dataset_iter_path)\n",
    "\n",
    "if config[\"feedback\"][\"type\"] == 'scalar':\n",
    "    reward_learner = ScalarRewardLearner(\n",
    "        trajectory_generator=trajectory_generator,\n",
    "        reward_model=reward_net,\n",
    "        num_iterations=N_ITER,\n",
    "        fragmenter=fragmenter,\n",
    "        feedback_gatherer=gatherer,\n",
    "        feedback_queue_size=wandb.config[\"dataset_max_size\"],\n",
    "        reward_trainer=reward_trainer,\n",
    "        fragment_length=wandb.config[\"fragment_length\"],\n",
    "        transition_oversampling=wandb.config[\"transition_oversampling\"],\n",
    "        initial_epoch_multiplier=wandb.config[\"initial_epoch_multiplier\"],\n",
    "        policy_evaluator=policy_evaluator,\n",
    "        custom_logger=logger,\n",
    "        callback=save_model_params_and_dataset_callback,\n",
    "    )\n",
    "\n",
    "else:\n",
    "    reward_learner = PreferenceComparisons(\n",
    "        trajectory_generator=trajectory_generator,\n",
    "        reward_model=reward_net,\n",
    "        num_iterations=N_ITER,\n",
    "        fragmenter=fragmenter,\n",
    "        preference_gatherer=gatherer,\n",
    "        comparison_queue_size=wandb.config[\"dataset_max_size\"],\n",
    "        reward_trainer=reward_trainer,\n",
    "        fragment_length=wandb.config[\"fragment_length\"],\n",
    "        transition_oversampling=wandb.config[\"transition_oversampling\"],\n",
    "        initial_epoch_multiplier=wandb.config[\"initial_epoch_multiplier\"],\n",
    "        initial_comparison_frac=0.1,\n",
    "        query_schedule=\"hyperbolic\",\n",
    "        policy_evaluator=policy_evaluator,\n",
    "        custom_logger=logger,\n",
    "    )\n",
    "\n",
    "#######################################################################################################################\n",
    "####################################################### Training ######################################################\n",
    "#######################################################################################################################\n",
    "\n",
    "if config[\"feedback\"][\"type\"] == 'scalar':\n",
    "    result = reward_learner.train(\n",
    "        # Just needs to be bigger then N_ITER * HORIZON. Value iteration doesn't really use this.\n",
    "        total_timesteps=10 * N_ITER * wandb.config[\"environment\"][\"horizon\"],\n",
    "        total_queries=N_COMPARISONS,\n",
    "    )\n",
    "\n",
    "else:\n",
    "    result = reward_learner.train(\n",
    "        # Just needs to be bigger then N_ITER * HORIZON. Value iteration doesn't really use this.\n",
    "        total_timesteps=10 * N_ITER * wandb.config[\"environment\"][\"horizon\"],\n",
    "        total_comparisons=N_COMPARISONS,\n",
    "        callback=save_model_params_and_dataset_callback,\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Constructing state index: 100%|██████████| 196900/196900 [00:05<00:00, 34066.04it/s]\n"
     ]
    }
   ],
   "source": [
    "trajectories = trajectory_generator.sample(100)\n",
    "horizons = (len(traj) for traj in trajectories if traj.terminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carrying coin, breaking\n"
     ]
    }
   ],
   "source": [
    "for traj in trajectories:\n",
    "    for obs in traj.obs:\n",
    "        if np.any(obs[-1]):\n",
    "            print(\"Carrying coin, breaking\")\n",
    "            break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def render_gridworld(observation=None, trajectory=None, grid_size=5, delay=1.0):\n",
    "    \"\"\"\n",
    "    Simple ASCII rendering of the environment.\n",
    "    \n",
    "    Args:\n",
    "        observation (np.array): Observation space image with 5 channels.\n",
    "                                If provided, it will be used to render the state.\n",
    "        trajectory (list of np.array): List of observations to render as a trajectory.\n",
    "        grid_size (int): Size of the grid (default is 5).\n",
    "        delay (float): Delay in seconds between rendering consecutive frames in the trajectory.\n",
    "    \"\"\"\n",
    "    HOME = \"H\"\n",
    "    OWNED_PELLET = \"x\"\n",
    "    FREE_PELLET = \".\"\n",
    "    AGENT = \"A\"\n",
    "\n",
    "    def render_single_observation(obs):\n",
    "        if obs is not None:\n",
    "            agent_position = np.argwhere(obs[0, :, :] == 1)[0]\n",
    "            free_pellet_locations = np.argwhere(obs[1, :, :] == 1)\n",
    "            owned_pellet_locations = np.argwhere(obs[2, :, :] == 1)\n",
    "            home_location = np.argwhere(obs[3, :, :] == 1)[0]\n",
    "            num_carried_pellets = obs[4, 0, 0]  # Assumes carried pellets are the same across all pixels in the channel\n",
    "            agent_repr = str(num_carried_pellets)\n",
    "        else:\n",
    "            agent_position = None\n",
    "            free_pellet_locations = []\n",
    "            owned_pellet_locations = []\n",
    "            home_location = (0, 0)\n",
    "            num_carried_pellets = 0\n",
    "            agent_repr = AGENT\n",
    "\n",
    "        grid = np.full((grid_size, grid_size), \" \")\n",
    "        grid[home_location[0], home_location[1]] = HOME\n",
    "        for loc in free_pellet_locations:\n",
    "            grid[loc[0], loc[1]] = FREE_PELLET\n",
    "        for loc in owned_pellet_locations:\n",
    "            grid[loc[0], loc[1]] = OWNED_PELLET\n",
    "\n",
    "        print(\"+\" + \"---+\" * grid_size)\n",
    "        for i in range(grid_size):\n",
    "            print(\"|\", end=\"\")\n",
    "            for j in range(grid_size):\n",
    "                if agent_position is not None and agent_position[0] == i and agent_position[1] == j:\n",
    "                    print(f\"{agent_repr}{grid[i, j]} |\", end=\"\")\n",
    "                else:\n",
    "                    print(f\" {grid[i, j]} |\", end=\"\")\n",
    "            print(\"\\n+\" + \"---+\" * grid_size)\n",
    "\n",
    "    if trajectory is not None:\n",
    "        for step, obs in enumerate(trajectory.obs):\n",
    "            print(f\"Step {step + 1} of {len(trajectory)}\")\n",
    "            render_single_observation(obs)\n",
    "            time.sleep(delay)\n",
    "            print(\"\\n\" * 2)\n",
    "    else:\n",
    "        render_single_observation(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 of 30\n",
      "+---+---+---+---+---+\n",
      "|   | . |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   | x |0H |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   | . |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "\n",
      "\n",
      "\n",
      "Step 2 of 30\n",
      "+---+---+---+---+---+\n",
      "|   | . |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   | x | H |0  |   |\n",
      "+---+---+---+---+---+\n",
      "|   | . |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "\n",
      "\n",
      "\n",
      "Step 3 of 30\n",
      "+---+---+---+---+---+\n",
      "|   | . |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |0  |   |\n",
      "+---+---+---+---+---+\n",
      "|   | x | H |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   | . |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "\n",
      "\n",
      "\n",
      "Step 4 of 30\n",
      "+---+---+---+---+---+\n",
      "|   | . |   |0  |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   | x | H |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   | . |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "\n",
      "\n",
      "\n",
      "Step 5 of 30\n",
      "+---+---+---+---+---+\n",
      "|   | . |   |0  |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   | x | H |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   | . |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "\n",
      "\n",
      "\n",
      "Step 6 of 30\n",
      "+---+---+---+---+---+\n",
      "|   | . |   |0  |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   | x | H |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   | . |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "\n",
      "\n",
      "\n",
      "Step 7 of 30\n",
      "+---+---+---+---+---+\n",
      "|   | . |   |0  |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   | x | H |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   | . |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "\n",
      "\n",
      "\n",
      "Step 8 of 30\n",
      "+---+---+---+---+---+\n",
      "|   | . |0  |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   | x | H |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   | . |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "\n",
      "\n",
      "\n",
      "Step 9 of 30\n",
      "+---+---+---+---+---+\n",
      "|   | . |0  |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   | x | H |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   | . |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "\n",
      "\n",
      "\n",
      "Step 10 of 30\n",
      "+---+---+---+---+---+\n",
      "|   | . |0  |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   | x | H |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   | . |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "\n",
      "\n",
      "\n",
      "Step 11 of 30\n",
      "+---+---+---+---+---+\n",
      "|   |0. |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   | x | H |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   | . |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "\n",
      "\n",
      "\n",
      "Step 12 of 30\n",
      "+---+---+---+---+---+\n",
      "|   |1  |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   | x | H |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   | . |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "\n",
      "\n",
      "\n",
      "Step 13 of 30\n",
      "+---+---+---+---+---+\n",
      "|   |   |1  |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   | x | H |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   | . |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "\n",
      "\n",
      "\n",
      "Step 14 of 30\n",
      "+---+---+---+---+---+\n",
      "|   |   |1  |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   | x | H |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   | . |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "\n",
      "\n",
      "\n",
      "Step 15 of 30\n",
      "+---+---+---+---+---+\n",
      "|   |1  |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   | x | H |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   | . |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "\n",
      "\n",
      "\n",
      "Step 16 of 30\n",
      "+---+---+---+---+---+\n",
      "|   |   |1  |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   | x | H |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   | . |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "\n",
      "\n",
      "\n",
      "Step 17 of 30\n",
      "+---+---+---+---+---+\n",
      "|   |1  |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   | x | H |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   | . |   |   |   |\n",
      "+---+---+---+---+---+\n",
      "|   |   |   |   |   |\n",
      "+---+---+---+---+---+\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrender_gridworld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrajectory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2.0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 57\u001b[0m, in \u001b[0;36mrender_gridworld\u001b[0;34m(observation, trajectory, grid_size, delay)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(trajectory)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     56\u001b[0m         render_single_observation(obs)\n\u001b[0;32m---> 57\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "render_gridworld(trajectory=traj, grid_size=5, delay=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5, 5)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traj.obs[10].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent is on a free pellet off mask\n",
      "Agent is picking up a  pellet off mask\n"
     ]
    }
   ],
   "source": [
    "#traj.obs[10], traj.acts[10]\n",
    "\n",
    "visibility_mask = np.array([\n",
    "    [0, 0, 0, 0, 0],\n",
    "    [0, 1, 1, 1, 0],\n",
    "    [0, 1, 1, 1, 0],\n",
    "    [0, 1, 1, 1, 0],\n",
    "    [0, 0, 0, 0, 0],\n",
    "])\n",
    "\n",
    "masked_obs = traj.obs[10][:-1,:,:] * (1 - visibility_mask)\n",
    "\n",
    "agent = masked_obs[0]\n",
    "free_pellets = masked_obs[1]\n",
    "owned_pellets = masked_obs[2]\n",
    "\n",
    "if np.any(agent * free_pellets):\n",
    "    print(\"Agent is on a free pellet off mask\")\n",
    "    if traj.acts[10] == 4:\n",
    "        print(\"Agent is picking up a  pellet off mask\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,\n",
       " array([[1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1]], dtype=int16))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traj.acts[10], traj.obs[11][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'chao': 120}\n",
    "d2 = {\"hello\": 1}\n",
    "\n",
    "d.update(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chao': 120, 'hello': 1}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments = fragmenter(trajectories, 10, 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preferences = gatherer(fragments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(preferences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trajectories[0].rews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories[0].obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragments[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_generator.env.rollout_with_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randint((13,5,5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.random.randint(2, size = (13,5,5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[:, :-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randint(2, size=(10))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "m = np.random.randint(1, size=(13,5,5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_visible = m[:-1, 0].any(axis=(1, 2))\n",
    "agent_visible.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m[:, np.newaxis].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = [((1,2), (3,4)), ((5,6), (7,8)), ((9,10), (11,12)), ((13,14), (15,16))]\n",
    "\n",
    "# get all the first tuples in each tuple pair\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, item in enumerate(iter(list)):\n",
    "    print(index, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limits = [((12, 24), (8, 20)), ((3, 15), (1, 13)), ((13, 25), (14, 26)), ((9, 21), (7, 19)), ((7, 19), (15, 27)), ((1, 13), (10, 22)), ((5, 17), (12, 24)), ((1, 13), (18, 30)), ((14, 26), (3, 15)), ((10, 22), (2, 14)), ((6, 18), (18, 30)), ((1, 13), (8, 20)), ((13, 25), (9, 21)), ((17, 29), (8, 20)), ((1, 13), (5, 17)), ((15, 27), (7, 19)), ((12, 24), (2, 14)), ((15, 27), (14, 26)), ((5, 17), (14, 26)), ((0, 12), (2, 14)), ((12, 24), (8, 20)), ((10, 22), (12, 24)), ((15, 27), (5, 17)), ((18, 30), (4, 16)), ((0, 12), (4, 16)), ((17, 29), (5, 17)), ((9, 21), (14, 26)), ((7, 19), (15, 27)), ((2, 14), (1, 13)), ((13, 25), (3, 15)), ((9, 21), (13, 25)), ((4, 16), (5, 17)), ((18, 30), (1, 13)), ((6, 18), (17, 29)), ((14, 26), (18, 30)), ((14, 26), (8, 20)), ((8, 20), (17, 29)), ((13, 25), (5, 17)), ((8, 20), (16, 28)), ((12, 24), (8, 20)), ((1, 13), (12, 24)), ((14, 26), (0, 12)), ((12, 24), (2, 14)), ((15, 27), (3, 15)), ((8, 20), (6, 18)), ((2, 14), (18, 30)), ((1, 13), (1, 13)), ((8, 20), (17, 29)), ((15, 27), (5, 17)), ((16, 28), (8, 20)), ((0, 12), (17, 29)), ((5, 17), (2, 14)), ((4, 16), (12, 24)), ((6, 18), (2, 14)), ((4, 16), (0, 12)), ((5, 17), (15, 27)), ((3, 15), (18, 30)), ((17, 29), (4, 16)), ((10, 22), (0, 12)), ((2, 14), (16, 28)), ((4, 16), (16, 28)), ((0, 12), (14, 26)), ((16, 28), (1, 13)), ((0, 12), (17, 29)), ((1, 13), (15, 27)), ((6, 18), (11, 23)), ((5, 17), (5, 17)), ((9, 21), (9, 21)), ((3, 15), (4, 16)), ((18, 30), (2, 14)), ((14, 26), (11, 23)), ((0, 12), (13, 25)), ((8, 20), (2, 14)), ((18, 30), (5, 17)), ((5, 17), (18, 30)), ((9, 21), (0, 12)), ((15, 27), (2, 14)), ((0, 12), (18, 30)), ((14, 26), (15, 27)), ((16, 28), (0, 12)), ((4, 16), (15, 27)), ((14, 26), (11, 23)), ((5, 17), (7, 19)), ((2, 14), (12, 24)), ((10, 22), (4, 16)), ((15, 27), (15, 27)), ((9, 21), (11, 23)), ((6, 18), (5, 17)), ((11, 23), (3, 15)), ((8, 20), (2, 14)), ((3, 15), (3, 15)), ((9, 21), (12, 24)), ((18, 30), (9, 21)), ((10, 22), (0, 12)), ((4, 16), (4, 16)), ((5, 17), (0, 12)), ((16, 28), (1, 13)), ((7, 19), (17, 29)), ((15, 27), (17, 29)), ((11, 23), (14, 26)), ((14, 26), (2, 14)), ((18, 30), (16, 28)), ((13, 25), (12, 24)), ((8, 20), (9, 21)), ((6, 18), (6, 18)), ((8, 20), (7, 19)), ((15, 27), (18, 30)), ((14, 26), (6, 18)), ((9, 21), (12, 24)), ((14, 26), (12, 24)), ((0, 12), (3, 15)), ((17, 29), (2, 14)), ((11, 23), (13, 25)), ((13, 25), (2, 14)), ((7, 19), (11, 23)), ((17, 29), (17, 29)), ((0, 12), (1, 13)), ((2, 14), (10, 22)), ((12, 24), (9, 21)), ((9, 21), (9, 21)), ((16, 28), (2, 14)), ((9, 21), (12, 24)), ((3, 15), (12, 24)), ((15, 27), (14, 26)), ((10, 22), (5, 17)), ((1, 13), (17, 29)), ((2, 14), (13, 25)), ((6, 18), (14, 26)), ((12, 24), (18, 30)), ((8, 20), (2, 14)), ((2, 14), (2, 14)), ((10, 22), (0, 12)), ((8, 20), (5, 17)), ((0, 12), (5, 17)), ((10, 22), (2, 14)), ((13, 25), (5, 17)), ((12, 24), (18, 30)), ((11, 23), (1, 13)), ((4, 16), (4, 16)), ((15, 27), (3, 15)), ((17, 29), (0, 12)), ((15, 27), (11, 23)), ((5, 17), (16, 28)), ((18, 30), (17, 29)), ((15, 27), (15, 27)), ((2, 14), (2, 14)), ((10, 22), (11, 23)), ((15, 27), (13, 25)), ((1, 13), (13, 25)), ((9, 21), (1, 13)), ((4, 16), (16, 28)), ((7, 19), (1, 13)), ((11, 23), (8, 20)), ((16, 28), (10, 22)), ((10, 22), (5, 17)), ((4, 16), (12, 24)), ((8, 20), (9, 21)), ((2, 14), (2, 14)), ((1, 13), (5, 17)), ((7, 19), (12, 24)), ((15, 27), (4, 16)), ((17, 29), (0, 12)), ((2, 14), (4, 16)), ((11, 23), (13, 25)), ((11, 23), (6, 18)), ((3, 15), (8, 20)), ((18, 30), (9, 21)), ((2, 14), (17, 29)), ((5, 17), (15, 27)), ((7, 19), (9, 21)), ((16, 28), (15, 27)), ((10, 22), (6, 18)), ((1, 13), (14, 26)), ((13, 25), (16, 28)), ((4, 16), (11, 23)), ((9, 21), (18, 30)), ((8, 20), (5, 17)), ((14, 26), (7, 19)), ((12, 24), (3, 15)), ((13, 25), (3, 15)), ((18, 30), (10, 22)), ((12, 24), (2, 14)), ((6, 18), (1, 13)), ((9, 21), (8, 20)), ((4, 16), (17, 29)), ((3, 15), (5, 17)), ((13, 25), (9, 21)), ((15, 27), (16, 28)), ((7, 19), (12, 24)), ((13, 25), (6, 18)), ((17, 29), (16, 28)), ((13, 25), (16, 28)), ((5, 17), (11, 23)), ((11, 23), (9, 21)), ((0, 12), (7, 19)), ((7, 19), (3, 15)), ((4, 16), (0, 12)), ((7, 19), (5, 17)), ((7, 19), (5, 17)), ((13, 25), (18, 30)), ((18, 30), (14, 26)), ((5, 17), (18, 30)), ((1, 13), (2, 14)), ((12, 24), (10, 22)), ((2, 14), (18, 30)), ((18, 30), (14, 26)), ((5, 17), (0, 12)), ((16, 28), (18, 30)), ((4, 16), (7, 19)), ((4, 16), (10, 22)), ((8, 20), (15, 27)), ((18, 30), (0, 12)), ((14, 26), (13, 25)), ((14, 26), (14, 26)), ((17, 29), (4, 16)), ((8, 20), (0, 12)), ((10, 22), (0, 12)), ((11, 23), (12, 24)), ((9, 21), (6, 18)), ((13, 25), (11, 23)), ((10, 22), (16, 28)), ((12, 24), (6, 18)), ((10, 22), (18, 30)), ((10, 22), (2, 14)), ((16, 28), (13, 25)), ((9, 21), (17, 29)), ((1, 13), (6, 18)), ((10, 22), (0, 12)), ((16, 28), (6, 18)), ((18, 30), (14, 26)), ((6, 18), (17, 29)), ((16, 28), (17, 29)), ((4, 16), (11, 23)), ((16, 28), (4, 16)), ((6, 18), (16, 28)), ((4, 16), (15, 27)), ((16, 28), (9, 21)), ((4, 16), (15, 27)), ((8, 20), (3, 15)), ((8, 20), (12, 24)), ((17, 29), (8, 20)), ((14, 26), (1, 13)), ((10, 22), (1, 13)), ((15, 27), (5, 17)), ((12, 24), (7, 19)), ((8, 20), (3, 15)), ((4, 16), (7, 19)), ((7, 19), (16, 28)), ((5, 17), (9, 21)), ((10, 22), (14, 26)), ((4, 16), (6, 18)), ((16, 28), (12, 24)), ((4, 16), (17, 29)), ((16, 28), (13, 25)), ((6, 18), (2, 14)), ((17, 29), (3, 15)), ((13, 25), (11, 23)), ((4, 16), (11, 23)), ((5, 17), (16, 28)), ((17, 29), (14, 26)), ((4, 16), (17, 29)), ((17, 29), (18, 30)), ((2, 14), (6, 18)), ((5, 17), (4, 16)), ((15, 27), (1, 13)), ((7, 19), (0, 12)), ((16, 28), (13, 25)), ((2, 14), (16, 28)), ((14, 26), (2, 14)), ((15, 27), (9, 21)), ((17, 29), (0, 12)), ((7, 19), (2, 14)), ((7, 19), (10, 22)), ((13, 25), (0, 12)), ((6, 18), (12, 24)), ((0, 12), (11, 23)), ((15, 27), (4, 16)), ((9, 21), (7, 19)), ((14, 26), (10, 22)), ((2, 14), (7, 19)), ((0, 12), (15, 27)), ((17, 29), (1, 13)), ((3, 15), (12, 24)), ((5, 17), (3, 15)), ((18, 30), (17, 29)), ((16, 28), (13, 25)), ((7, 19), (9, 21)), ((10, 22), (14, 26)), ((14, 26), (17, 29)), ((9, 21), (1, 13)), ((8, 20), (0, 12)), ((15, 27), (13, 25)), ((11, 23), (6, 18)), ((7, 19), (7, 19)), ((9, 21), (8, 20)), ((6, 18), (7, 19)), ((5, 17), (13, 25)), ((13, 25), (4, 16)), ((11, 23), (16, 28)), ((18, 30), (3, 15))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limits_f1 = [pair[0] for pair in limits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limits_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "np.random.choice(np.random.randint(0,100), 10, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#home_location_raveled = np.ravel_multi_index(self.home_location, (self.grid_size, self.grid_size))\n",
    "\n",
    "np.ravel_multi_index((2,2), (5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "c = 0\n",
    "for i in itertools.product(range(5), repeat=2):\n",
    "    c += 1\n",
    "    print(i)\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = np.random.randint(1, size=(10, 5, 5, 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs[:, -1, :, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "carried_across_time = np.arange(10)\n",
    "pickups = np.diff(carried_across_time)\n",
    "agent_vis = np.random.randint(2, size=10)\n",
    "\n",
    "print(\"carried_across_time = \", carried_across_time)\n",
    "print(\"pickups = \", pickups)\n",
    "print(\"agent_vis = \", agent_vis)\n",
    "\n",
    "np.where(agent_vis[:-1] == 1, 0, pickups)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.diff(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RLHF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
