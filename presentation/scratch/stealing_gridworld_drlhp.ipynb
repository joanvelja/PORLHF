{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as th\n",
    "\n",
    "from imitation.algorithms import preference_comparisons\n",
    "\n",
    "from imitation_modules import DeterministicMDPTrajGenerator, NonImageCnnRewardNet, SyntheticValueGatherer\n",
    "from stealing_gridworld import StealingGridworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enumerating states: 100%|██████████| 9/9 [00:00<00:00, 112.49it/s]\n",
      "Constructing transition matrix: 100%|██████████| 3492/3492 [00:02<00:00, 1524.75it/s]\n",
      "Value iteration: 100%|██████████| 20/20 [00:00<00:00, 1157.21it/s]\n"
     ]
    }
   ],
   "source": [
    "GRID_SIZE = 3\n",
    "HORIZON = 20\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "env = StealingGridworld(\n",
    "    grid_size=GRID_SIZE,\n",
    "    max_steps=HORIZON,\n",
    "    reward_for_stealing=-200,\n",
    ")\n",
    "\n",
    "reward_net = NonImageCnnRewardNet(\n",
    "    env.observation_space,\n",
    "    env.action_space,\n",
    "    hid_channels=(32,32),\n",
    "    kernel_size=3,\n",
    ")\n",
    "\n",
    "fragmenter = preference_comparisons.RandomFragmenter(warning_threshold=0, rng=rng)\n",
    "gatherer = SyntheticValueGatherer(\n",
    "    env,\n",
    "    # temperature=0,\n",
    "    rng=rng,\n",
    "    value_coeff=0.01,\n",
    ")\n",
    "preference_model = preference_comparisons.PreferenceModel(reward_net)\n",
    "reward_trainer = preference_comparisons.BasicRewardTrainer(\n",
    "    preference_model=preference_model,\n",
    "    loss=preference_comparisons.CrossEntropyRewardLoss(),\n",
    "    epochs=3,\n",
    "    rng=rng,\n",
    ")\n",
    "trajectory_generator = DeterministicMDPTrajGenerator(\n",
    "    reward_fn=reward_net,\n",
    "    env=env,\n",
    "    rng=None,\n",
    "    epsilon=0.1,\n",
    "    vi_gamma=0.5,\n",
    ")\n",
    "pref_comparisons = preference_comparisons.PreferenceComparisons(\n",
    "    trajectory_generator,\n",
    "    reward_net,\n",
    "    num_iterations=20,\n",
    "    fragmenter=fragmenter,\n",
    "    preference_gatherer=gatherer,\n",
    "    reward_trainer=reward_trainer,\n",
    "    fragment_length=1,\n",
    "    # initial_comparison_frac=0.01,\n",
    "    initial_epoch_multiplier=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query schedule: [1000, 647, 614, 585, 558, 534, 512, 491, 472, 455, 439, 424, 409, 396, 384, 372, 361, 351, 341, 332, 323]\n",
      "Collecting 2000 fragments (2000 transitions)\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 1000 comparisons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training reward model: 100%|██████████| 30/30 [01:45<00:00,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 2500 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Value iteration: 100%|██████████| 20/20 [00:00<00:00, 1198.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "| mean/                                   |          |\n",
      "|    preferences/entropy                  | 0.692    |\n",
      "|    reward/epoch-0/train/accuracy        | 0.514    |\n",
      "|    reward/epoch-0/train/gt_reward_loss  | 0.692    |\n",
      "|    reward/epoch-0/train/loss            | 0.69     |\n",
      "|    reward/epoch-1/train/accuracy        | 0.56     |\n",
      "|    reward/epoch-1/train/gt_reward_loss  | 0.692    |\n",
      "|    reward/epoch-1/train/loss            | 0.679    |\n",
      "|    reward/epoch-10/train/accuracy       | 0.641    |\n",
      "|    reward/epoch-10/train/gt_reward_loss | 0.692    |\n",
      "|    reward/epoch-10/train/loss           | 0.646    |\n",
      "|    reward/epoch-11/train/accuracy       | 0.649    |\n",
      "|    reward/epoch-11/train/gt_reward_loss | 0.692    |\n",
      "|    reward/epoch-11/train/loss           | 0.639    |\n",
      "|    reward/epoch-12/train/accuracy       | 0.661    |\n",
      "|    reward/epoch-12/train/gt_reward_loss | 0.692    |\n",
      "|    reward/epoch-12/train/loss           | 0.64     |\n",
      "|    reward/epoch-13/train/accuracy       | 0.655    |\n",
      "|    reward/epoch-13/train/gt_reward_loss | 0.692    |\n",
      "|    reward/epoch-13/train/loss           | 0.639    |\n",
      "|    reward/epoch-14/train/accuracy       | 0.651    |\n",
      "|    reward/epoch-14/train/gt_reward_loss | 0.692    |\n",
      "|    reward/epoch-14/train/loss           | 0.635    |\n",
      "|    reward/epoch-15/train/accuracy       | 0.666    |\n",
      "|    reward/epoch-15/train/gt_reward_loss | 0.692    |\n",
      "|    reward/epoch-15/train/loss           | 0.628    |\n",
      "|    reward/epoch-16/train/accuracy       | 0.66     |\n",
      "|    reward/epoch-16/train/gt_reward_loss | 0.692    |\n",
      "|    reward/epoch-16/train/loss           | 0.632    |\n",
      "|    reward/epoch-17/train/accuracy       | 0.664    |\n",
      "|    reward/epoch-17/train/gt_reward_loss | 0.692    |\n",
      "|    reward/epoch-17/train/loss           | 0.632    |\n",
      "|    reward/epoch-18/train/accuracy       | 0.643    |\n",
      "|    reward/epoch-18/train/gt_reward_loss | 0.692    |\n",
      "|    reward/epoch-18/train/loss           | 0.632    |\n",
      "|    reward/epoch-19/train/accuracy       | 0.646    |\n",
      "|    reward/epoch-19/train/gt_reward_loss | 0.692    |\n",
      "|    reward/epoch-19/train/loss           | 0.63     |\n",
      "|    reward/epoch-2/train/accuracy        | 0.571    |\n",
      "|    reward/epoch-2/train/gt_reward_loss  | 0.692    |\n",
      "|    reward/epoch-2/train/loss            | 0.672    |\n",
      "|    reward/epoch-20/train/accuracy       | 0.672    |\n",
      "|    reward/epoch-20/train/gt_reward_loss | 0.692    |\n",
      "|    reward/epoch-20/train/loss           | 0.622    |\n",
      "|    reward/epoch-21/train/accuracy       | 0.648    |\n",
      "|    reward/epoch-21/train/gt_reward_loss | 0.692    |\n",
      "|    reward/epoch-21/train/loss           | 0.627    |\n",
      "|    reward/epoch-22/train/accuracy       | 0.664    |\n",
      "|    reward/epoch-22/train/gt_reward_loss | 0.692    |\n",
      "|    reward/epoch-22/train/loss           | 0.623    |\n",
      "|    reward/epoch-23/train/accuracy       | 0.67     |\n",
      "|    reward/epoch-23/train/gt_reward_loss | 0.692    |\n",
      "|    reward/epoch-23/train/loss           | 0.618    |\n",
      "|    reward/epoch-24/train/accuracy       | 0.66     |\n",
      "|    reward/epoch-24/train/gt_reward_loss | 0.692    |\n",
      "|    reward/epoch-24/train/loss           | 0.615    |\n",
      "|    reward/epoch-25/train/accuracy       | 0.651    |\n",
      "|    reward/epoch-25/train/gt_reward_loss | 0.692    |\n",
      "|    reward/epoch-25/train/loss           | 0.617    |\n",
      "|    reward/epoch-26/train/accuracy       | 0.676    |\n",
      "|    reward/epoch-26/train/gt_reward_loss | 0.692    |\n",
      "|    reward/epoch-26/train/loss           | 0.612    |\n",
      "|    reward/epoch-27/train/accuracy       | 0.678    |\n",
      "|    reward/epoch-27/train/gt_reward_loss | 0.692    |\n",
      "|    reward/epoch-27/train/loss           | 0.612    |\n",
      "|    reward/epoch-28/train/accuracy       | 0.675    |\n",
      "|    reward/epoch-28/train/gt_reward_loss | 0.692    |\n",
      "|    reward/epoch-28/train/loss           | 0.61     |\n",
      "|    reward/epoch-29/train/accuracy       | 0.671    |\n",
      "|    reward/epoch-29/train/gt_reward_loss | 0.692    |\n",
      "|    reward/epoch-29/train/loss           | 0.614    |\n",
      "|    reward/epoch-3/train/accuracy        | 0.599    |\n",
      "|    reward/epoch-3/train/gt_reward_loss  | 0.692    |\n",
      "|    reward/epoch-3/train/loss            | 0.669    |\n",
      "|    reward/epoch-4/train/accuracy        | 0.593    |\n",
      "|    reward/epoch-4/train/gt_reward_loss  | 0.692    |\n",
      "|    reward/epoch-4/train/loss            | 0.666    |\n",
      "|    reward/epoch-5/train/accuracy        | 0.605    |\n",
      "|    reward/epoch-5/train/gt_reward_loss  | 0.692    |\n",
      "|    reward/epoch-5/train/loss            | 0.664    |\n",
      "|    reward/epoch-6/train/accuracy        | 0.625    |\n",
      "|    reward/epoch-6/train/gt_reward_loss  | 0.692    |\n",
      "|    reward/epoch-6/train/loss            | 0.657    |\n",
      "|    reward/epoch-7/train/accuracy        | 0.624    |\n",
      "|    reward/epoch-7/train/gt_reward_loss  | 0.692    |\n",
      "|    reward/epoch-7/train/loss            | 0.657    |\n",
      "|    reward/epoch-8/train/accuracy        | 0.618    |\n",
      "|    reward/epoch-8/train/gt_reward_loss  | 0.692    |\n",
      "|    reward/epoch-8/train/loss            | 0.66     |\n",
      "|    reward/epoch-9/train/accuracy        | 0.633    |\n",
      "|    reward/epoch-9/train/gt_reward_loss  | 0.692    |\n",
      "|    reward/epoch-9/train/loss            | 0.647    |\n",
      "| reward/                                 |          |\n",
      "|    final/train/accuracy                 | 0.671    |\n",
      "|    final/train/gt_reward_loss           | 0.692    |\n",
      "|    final/train/loss                     | 0.614    |\n",
      "------------------------------------------------------\n",
      "Collecting 1294 fragments (1294 transitions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 1647 comparisons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training reward model: 100%|██████████| 3/3 [00:14<00:00,  4.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 2500 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Value iteration: 100%|██████████| 20/20 [00:00<00:00, 1233.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    preferences/entropy                 | 0.69     |\n",
      "|    reward/epoch-0/train/accuracy       | 0.614    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.691    |\n",
      "|    reward/epoch-0/train/loss           | 0.651    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.618    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.691    |\n",
      "|    reward/epoch-1/train/loss           | 0.643    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.634    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.691    |\n",
      "|    reward/epoch-2/train/loss           | 0.638    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.634    |\n",
      "|    final/train/gt_reward_loss          | 0.691    |\n",
      "|    final/train/loss                    | 0.638    |\n",
      "-----------------------------------------------------\n",
      "Collecting 1228 fragments (1228 transitions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 2261 comparisons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training reward model: 100%|██████████| 3/3 [00:21<00:00,  7.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 2500 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Value iteration: 100%|██████████| 20/20 [00:00<00:00, 1218.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    preferences/entropy                 | 0.685    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.608    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.689    |\n",
      "|    reward/epoch-0/train/loss           | 0.651    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.629    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.689    |\n",
      "|    reward/epoch-1/train/loss           | 0.643    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.633    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.689    |\n",
      "|    reward/epoch-2/train/loss           | 0.639    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.633    |\n",
      "|    final/train/gt_reward_loss          | 0.689    |\n",
      "|    final/train/loss                    | 0.639    |\n",
      "-----------------------------------------------------\n",
      "Collecting 1170 fragments (1170 transitions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 2846 comparisons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training reward model: 100%|██████████| 3/3 [00:26<00:00,  8.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 2500 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Value iteration: 100%|██████████| 20/20 [00:00<00:00, 1229.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    preferences/entropy                 | 0.672    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.61     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.686    |\n",
      "|    reward/epoch-0/train/loss           | 0.653    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.619    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.686    |\n",
      "|    reward/epoch-1/train/loss           | 0.646    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.627    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.686    |\n",
      "|    reward/epoch-2/train/loss           | 0.644    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.627    |\n",
      "|    final/train/gt_reward_loss          | 0.686    |\n",
      "|    final/train/loss                    | 0.644    |\n",
      "-----------------------------------------------------\n",
      "Collecting 1116 fragments (1116 transitions)\n",
      "Creating fragment pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering preferences\n",
      "Dataset now contains 3404 comparisons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training reward model: 100%|██████████| 3/3 [00:30<00:00, 10.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 2500 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Value iteration: 100%|██████████| 20/20 [00:00<00:00, 1220.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    preferences/entropy                 | 0.682    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.606    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.685    |\n",
      "|    reward/epoch-0/train/loss           | 0.654    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.617    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.685    |\n",
      "|    reward/epoch-1/train/loss           | 0.651    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.621    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.685    |\n",
      "|    reward/epoch-2/train/loss           | 0.647    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.621    |\n",
      "|    final/train/gt_reward_loss          | 0.685    |\n",
      "|    final/train/loss                    | 0.647    |\n",
      "-----------------------------------------------------\n",
      "Collecting 1068 fragments (1068 transitions)\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset now contains 3938 comparisons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training reward model: 100%|██████████| 3/3 [00:34<00:00, 11.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 2500 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Value iteration: 100%|██████████| 20/20 [00:00<00:00, 1237.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    preferences/entropy                 | 0.691    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.599    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.686    |\n",
      "|    reward/epoch-0/train/loss           | 0.657    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.613    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.686    |\n",
      "|    reward/epoch-1/train/loss           | 0.653    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.62     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.686    |\n",
      "|    reward/epoch-2/train/loss           | 0.647    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.62     |\n",
      "|    final/train/gt_reward_loss          | 0.686    |\n",
      "|    final/train/loss                    | 0.647    |\n",
      "-----------------------------------------------------\n",
      "Collecting 1024 fragments (1024 transitions)\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset now contains 4450 comparisons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training reward model: 100%|██████████| 3/3 [00:40<00:00, 13.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 2500 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Value iteration: 100%|██████████| 20/20 [00:00<00:00, 1219.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    preferences/entropy                 | 0.688    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.609    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.686    |\n",
      "|    reward/epoch-0/train/loss           | 0.655    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.612    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.686    |\n",
      "|    reward/epoch-1/train/loss           | 0.652    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.619    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.686    |\n",
      "|    reward/epoch-2/train/loss           | 0.65     |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.619    |\n",
      "|    final/train/gt_reward_loss          | 0.686    |\n",
      "|    final/train/loss                    | 0.65     |\n",
      "-----------------------------------------------------\n",
      "Collecting 982 fragments (982 transitions)\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset now contains 4941 comparisons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training reward model: 100%|██████████| 3/3 [00:45<00:00, 15.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 2500 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Value iteration: 100%|██████████| 20/20 [00:00<00:00, 1222.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    preferences/entropy                 | 0.678    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.606    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.685    |\n",
      "|    reward/epoch-0/train/loss           | 0.655    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.615    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.685    |\n",
      "|    reward/epoch-1/train/loss           | 0.651    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.619    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.685    |\n",
      "|    reward/epoch-2/train/loss           | 0.647    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.619    |\n",
      "|    final/train/gt_reward_loss          | 0.685    |\n",
      "|    final/train/loss                    | 0.647    |\n",
      "-----------------------------------------------------\n",
      "Collecting 944 fragments (944 transitions)\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset now contains 5413 comparisons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training reward model: 100%|██████████| 3/3 [00:51<00:00, 17.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 2500 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Value iteration: 100%|██████████| 20/20 [00:00<00:00, 1220.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    preferences/entropy                 | 0.693    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.61     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.686    |\n",
      "|    reward/epoch-0/train/loss           | 0.653    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.62     |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.686    |\n",
      "|    reward/epoch-1/train/loss           | 0.65     |\n",
      "|    reward/epoch-2/train/accuracy       | 0.624    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.685    |\n",
      "|    reward/epoch-2/train/loss           | 0.646    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.624    |\n",
      "|    final/train/gt_reward_loss          | 0.685    |\n",
      "|    final/train/loss                    | 0.646    |\n",
      "-----------------------------------------------------\n",
      "Collecting 910 fragments (910 transitions)\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset now contains 5868 comparisons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training reward model: 100%|██████████| 3/3 [00:54<00:00, 18.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 2500 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Value iteration: 100%|██████████| 20/20 [00:00<00:00, 1170.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    preferences/entropy                 | 0.686    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.615    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.686    |\n",
      "|    reward/epoch-0/train/loss           | 0.651    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.622    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.686    |\n",
      "|    reward/epoch-1/train/loss           | 0.647    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.621    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.686    |\n",
      "|    reward/epoch-2/train/loss           | 0.643    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.621    |\n",
      "|    final/train/gt_reward_loss          | 0.686    |\n",
      "|    final/train/loss                    | 0.643    |\n",
      "-----------------------------------------------------\n",
      "Collecting 878 fragments (878 transitions)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 6307 comparisons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training reward model: 100%|██████████| 3/3 [01:00<00:00, 20.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 2500 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Value iteration: 100%|██████████| 20/20 [00:00<00:00, 1225.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    preferences/entropy                 | 0.688    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.614    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.686    |\n",
      "|    reward/epoch-0/train/loss           | 0.647    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.619    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.686    |\n",
      "|    reward/epoch-1/train/loss           | 0.644    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.621    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.686    |\n",
      "|    reward/epoch-2/train/loss           | 0.643    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.621    |\n",
      "|    final/train/gt_reward_loss          | 0.686    |\n",
      "|    final/train/loss                    | 0.643    |\n",
      "-----------------------------------------------------\n",
      "Collecting 848 fragments (848 transitions)\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset now contains 6731 comparisons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training reward model: 100%|██████████| 3/3 [01:04<00:00, 21.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 2500 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Value iteration: 100%|██████████| 20/20 [00:00<00:00, 1188.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    preferences/entropy                 | 0.693    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.62     |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.687    |\n",
      "|    reward/epoch-0/train/loss           | 0.646    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.622    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.687    |\n",
      "|    reward/epoch-1/train/loss           | 0.644    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.623    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.687    |\n",
      "|    reward/epoch-2/train/loss           | 0.641    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.623    |\n",
      "|    final/train/gt_reward_loss          | 0.687    |\n",
      "|    final/train/loss                    | 0.641    |\n",
      "-----------------------------------------------------\n",
      "Collecting 818 fragments (818 transitions)\n",
      "Creating fragment pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering preferences\n",
      "Dataset now contains 7140 comparisons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training reward model: 100%|██████████| 3/3 [01:14<00:00, 24.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 2500 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Value iteration: 100%|██████████| 20/20 [00:00<00:00, 1240.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    preferences/entropy                 | 0.693    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.617    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.687    |\n",
      "|    reward/epoch-0/train/loss           | 0.645    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.617    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.687    |\n",
      "|    reward/epoch-1/train/loss           | 0.642    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.624    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.687    |\n",
      "|    reward/epoch-2/train/loss           | 0.64     |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.624    |\n",
      "|    final/train/gt_reward_loss          | 0.687    |\n",
      "|    final/train/loss                    | 0.64     |\n",
      "-----------------------------------------------------\n",
      "Collecting 792 fragments (792 transitions)\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset now contains 7536 comparisons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training reward model: 100%|██████████| 3/3 [01:13<00:00, 24.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 2500 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Value iteration: 100%|██████████| 20/20 [00:00<00:00, 1185.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    preferences/entropy                 | 0.693    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.624    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.687    |\n",
      "|    reward/epoch-0/train/loss           | 0.644    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.622    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.687    |\n",
      "|    reward/epoch-1/train/loss           | 0.642    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.62     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.687    |\n",
      "|    reward/epoch-2/train/loss           | 0.641    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.62     |\n",
      "|    final/train/gt_reward_loss          | 0.687    |\n",
      "|    final/train/loss                    | 0.641    |\n",
      "-----------------------------------------------------\n",
      "Collecting 768 fragments (768 transitions)\n",
      "Creating fragment pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering preferences\n",
      "Dataset now contains 7920 comparisons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training reward model: 100%|██████████| 3/3 [01:18<00:00, 26.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 2500 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Value iteration: 100%|██████████| 20/20 [00:00<00:00, 1212.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    preferences/entropy                 | 0.691    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.618    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.687    |\n",
      "|    reward/epoch-0/train/loss           | 0.644    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.619    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.687    |\n",
      "|    reward/epoch-1/train/loss           | 0.643    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.631    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.687    |\n",
      "|    reward/epoch-2/train/loss           | 0.641    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.631    |\n",
      "|    final/train/gt_reward_loss          | 0.687    |\n",
      "|    final/train/loss                    | 0.641    |\n",
      "-----------------------------------------------------\n",
      "Collecting 744 fragments (744 transitions)\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset now contains 8292 comparisons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training reward model: 100%|██████████| 3/3 [01:17<00:00, 25.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 2500 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Value iteration: 100%|██████████| 20/20 [00:00<00:00, 1218.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    preferences/entropy                 | 0.693    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.618    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.688    |\n",
      "|    reward/epoch-0/train/loss           | 0.643    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.621    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.688    |\n",
      "|    reward/epoch-1/train/loss           | 0.642    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.621    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.688    |\n",
      "|    reward/epoch-2/train/loss           | 0.639    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.621    |\n",
      "|    final/train/gt_reward_loss          | 0.688    |\n",
      "|    final/train/loss                    | 0.639    |\n",
      "-----------------------------------------------------\n",
      "Collecting 722 fragments (722 transitions)\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset now contains 8653 comparisons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training reward model: 100%|██████████| 3/3 [01:27<00:00, 29.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 2500 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Value iteration: 100%|██████████| 20/20 [00:00<00:00, 1208.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    preferences/entropy                 | 0.689    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.622    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.688    |\n",
      "|    reward/epoch-0/train/loss           | 0.644    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.624    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.688    |\n",
      "|    reward/epoch-1/train/loss           | 0.639    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.627    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.688    |\n",
      "|    reward/epoch-2/train/loss           | 0.639    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.627    |\n",
      "|    final/train/gt_reward_loss          | 0.688    |\n",
      "|    final/train/loss                    | 0.639    |\n",
      "-----------------------------------------------------\n",
      "Collecting 702 fragments (702 transitions)\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 9004 comparisons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training reward model: 100%|██████████| 3/3 [01:33<00:00, 31.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 2500 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Value iteration: 100%|██████████| 20/20 [00:00<00:00, 1215.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    preferences/entropy                 | 0.691    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.624    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.688    |\n",
      "|    reward/epoch-0/train/loss           | 0.642    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.623    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.688    |\n",
      "|    reward/epoch-1/train/loss           | 0.641    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.62     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.688    |\n",
      "|    reward/epoch-2/train/loss           | 0.639    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.62     |\n",
      "|    final/train/gt_reward_loss          | 0.688    |\n",
      "|    final/train/loss                    | 0.639    |\n",
      "-----------------------------------------------------\n",
      "Collecting 682 fragments (682 transitions)\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 9345 comparisons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training reward model: 100%|██████████| 3/3 [01:26<00:00, 28.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 2500 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Value iteration: 100%|██████████| 20/20 [00:00<00:00, 1194.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    preferences/entropy                 | 0.683    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.625    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.688    |\n",
      "|    reward/epoch-0/train/loss           | 0.639    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.626    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.688    |\n",
      "|    reward/epoch-1/train/loss           | 0.639    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.628    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.688    |\n",
      "|    reward/epoch-2/train/loss           | 0.637    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.628    |\n",
      "|    final/train/gt_reward_loss          | 0.688    |\n",
      "|    final/train/loss                    | 0.637    |\n",
      "-----------------------------------------------------\n",
      "Collecting 664 fragments (664 transitions)\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n",
      "Dataset now contains 9677 comparisons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training reward model: 100%|██████████| 3/3 [01:35<00:00, 31.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 2500 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Value iteration: 100%|██████████| 20/20 [00:00<00:00, 1170.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    preferences/entropy                 | 0.691    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.624    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.688    |\n",
      "|    reward/epoch-0/train/loss           | 0.64     |\n",
      "|    reward/epoch-1/train/accuracy       | 0.627    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.688    |\n",
      "|    reward/epoch-1/train/loss           | 0.636    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.625    |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.688    |\n",
      "|    reward/epoch-2/train/loss           | 0.638    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.625    |\n",
      "|    final/train/gt_reward_loss          | 0.688    |\n",
      "|    final/train/loss                    | 0.638    |\n",
      "-----------------------------------------------------\n",
      "Collecting 646 fragments (646 transitions)\n",
      "Creating fragment pairs\n",
      "Gathering preferences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset now contains 10000 comparisons\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training reward model: 100%|██████████| 3/3 [01:44<00:00, 34.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training agent for 2500 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Value iteration: 100%|██████████| 20/20 [00:00<00:00, 1141.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "| mean/                                  |          |\n",
      "|    preferences/entropy                 | 0.687    |\n",
      "|    reward/epoch-0/train/accuracy       | 0.622    |\n",
      "|    reward/epoch-0/train/gt_reward_loss | 0.688    |\n",
      "|    reward/epoch-0/train/loss           | 0.641    |\n",
      "|    reward/epoch-1/train/accuracy       | 0.623    |\n",
      "|    reward/epoch-1/train/gt_reward_loss | 0.688    |\n",
      "|    reward/epoch-1/train/loss           | 0.637    |\n",
      "|    reward/epoch-2/train/accuracy       | 0.62     |\n",
      "|    reward/epoch-2/train/gt_reward_loss | 0.688    |\n",
      "|    reward/epoch-2/train/loss           | 0.637    |\n",
      "| reward/                                |          |\n",
      "|    final/train/accuracy                | 0.62     |\n",
      "|    final/train/gt_reward_loss          | 0.688    |\n",
      "|    final/train/loss                    | 0.637    |\n",
      "-----------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result = pref_comparisons.train(\n",
    "    total_timesteps=50000,\n",
    "    total_comparisons=10000,\n",
    ")\n",
    "rlhf_policy = trajectory_generator.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|   |   |   |\n",
      "+---+---+---+\n",
      "|   |3H |   |\n",
      "+---+---+---+\n",
      "|   |   |   |\n",
      "+---+---+---+\n",
      "best action: RIGHT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.79211015, -2.5380538 ,  0.60095835,  2.4222856 ,  0.69246906],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What does the reward net think is the best state?\n",
    "\n",
    "_, reward_vector = env.get_sparse_transition_matrix_and_reward_vector(reward_net)\n",
    "\n",
    "best_state_idx, best_action = np.unravel_index(\n",
    "    reward_vector.argmax(),\n",
    "    (len(env.states), len(env.actions)),\n",
    ")\n",
    "env.render(env.states[best_state_idx])\n",
    "print(f\"best action: {env._action_to_string(env.actions[best_action])}\")\n",
    "\n",
    "reward_net.predict(\n",
    "    np.array([env.states[best_state_idx]] * 5),\n",
    "    np.array(np.arange(5)),\n",
    "    np.array(env.states[:5]),\n",
    "    np.array([False] * 5),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "| . |   |   |\n",
      "+---+---+---+\n",
      "|   |0H | . |\n",
      "+---+---+---+\n",
      "|   |   | x |\n",
      "+---+---+---+\n",
      "action: RIGHT\n",
      "Predicted reward: 0.5517340898513794\n",
      "+---+---+---+\n",
      "| . |   |   |\n",
      "+---+---+---+\n",
      "|   | H |0. |\n",
      "+---+---+---+\n",
      "|   |   | x |\n",
      "+---+---+---+\n",
      "action: RIGHT\n",
      "Predicted reward: 0.9585872292518616\n",
      "Repeated final state with action 3\n",
      "Total reward: 0\n"
     ]
    }
   ],
   "source": [
    "# Rollouts of the RLHF policy\n",
    "\n",
    "def log_reward_net_prediction(state, action, _):\n",
    "    reward_prection = reward_net.predict(\n",
    "        np.array([state]),\n",
    "        np.array([action]),\n",
    "        np.array([state]),\n",
    "        np.array([False]),\n",
    "    )[0]\n",
    "    print(f\"action: {env._action_to_string(action)}\")\n",
    "    print(f\"Predicted reward: {reward_prection}\")\n",
    "\n",
    "\n",
    "_ = env.rollout_with_policy(rlhf_policy, render=True, logging_callback=log_reward_net_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "| . |2  |   |\n",
      "+---+---+---+\n",
      "|   | H |   |\n",
      "+---+---+---+\n",
      "|   |   |   |\n",
      "+---+---+---+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([27.317883, 31.444069, 25.671495, 24.866295, 30.610226],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reward net evaluated at final state of above rollout\n",
    "\n",
    "state = env._get_observation()\n",
    "\n",
    "env.render(state)\n",
    "reward_net.predict(\n",
    "    np.array([state] * 5),\n",
    "    np.array(np.arange(5)),\n",
    "    np.array(env.states[:5]),\n",
    "    np.array([False] * 5),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "| . | x |   |\n",
      "+---+---+---+\n",
      "|   |1H |   |\n",
      "+---+---+---+\n",
      "|   |   |   |\n",
      "+---+---+---+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xk/tfsn_wq104x32hljn07w3ycr0000gn/T/ipykernel_58364/3894597419.py:18: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:248.)\n",
      "  conv0_activations = reward_net.cnn[0](th.Tensor([state]))\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 5 is out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m relu1_activations \u001b[39m=\u001b[39m reward_net\u001b[39m.\u001b[39mcnn[\u001b[39m3\u001b[39m](conv1_activations)\n\u001b[1;32m     22\u001b[0m pool_activations \u001b[39m=\u001b[39m reward_net\u001b[39m.\u001b[39mcnn[\u001b[39m4\u001b[39m](relu1_activations)\n\u001b[0;32m---> 23\u001b[0m flat_activations \u001b[39m=\u001b[39m reward_net\u001b[39m.\u001b[39;49mcnn[\u001b[39m5\u001b[39;49m](pool_activations)\n\u001b[1;32m     24\u001b[0m outputs \u001b[39m=\u001b[39m reward_net\u001b[39m.\u001b[39mcnn[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m](flat_activations)\n\u001b[1;32m     26\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlayer 0 activations:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mrelu0_activations[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/assisting_bounded_humans/lib/python3.9/site-packages/torch/nn/modules/container.py:120\u001b[0m, in \u001b[0;36mSequential.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m(OrderedDict(\u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_modules\u001b[39m.\u001b[39mitems())[idx]))\n\u001b[1;32m    119\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_item_by_idx(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_modules\u001b[39m.\u001b[39;49mvalues(), idx)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/assisting_bounded_humans/lib/python3.9/site-packages/torch/nn/modules/container.py:111\u001b[0m, in \u001b[0;36mSequential._get_item_by_idx\u001b[0;34m(self, iterator, idx)\u001b[0m\n\u001b[1;32m    109\u001b[0m idx \u001b[39m=\u001b[39m operator\u001b[39m.\u001b[39mindex(idx)\n\u001b[1;32m    110\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m-\u001b[39msize \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m idx \u001b[39m<\u001b[39m size:\n\u001b[0;32m--> 111\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mindex \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m is out of range\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(idx))\n\u001b[1;32m    112\u001b[0m idx \u001b[39m%\u001b[39m\u001b[39m=\u001b[39m size\n\u001b[1;32m    113\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(islice(iterator, idx, \u001b[39mNone\u001b[39;00m))\n",
      "\u001b[0;31mIndexError\u001b[0m: index 5 is out of range"
     ]
    }
   ],
   "source": [
    "# Trying to inspect activations of reward net\n",
    "\n",
    "import torch as th\n",
    "\n",
    "home_location = np.array([GRID_SIZE // 2, GRID_SIZE // 2])\n",
    "free_pellet_location = np.array([0, 0])\n",
    "owned_pellet_location = np.array([0, 1])\n",
    "\n",
    "agent_location = home_location\n",
    "\n",
    "state = np.zeros((5, GRID_SIZE, GRID_SIZE), dtype=np.int16)\n",
    "state[0, agent_location[0], agent_location[1]] = 1\n",
    "state[1, free_pellet_location[0], free_pellet_location[1]] = 1\n",
    "state[2, owned_pellet_location[0], owned_pellet_location[1]] = 1\n",
    "state[3, GRID_SIZE // 2, GRID_SIZE // 2] = 1\n",
    "state[4, :, :] = 1\n",
    "\n",
    "env.render(state)\n",
    "\n",
    "conv0_activations = reward_net.cnn[0](th.Tensor([state]))\n",
    "relu0_activations = reward_net.cnn[1](conv0_activations)\n",
    "conv1_activations = reward_net.cnn[2](relu0_activations)\n",
    "relu1_activations = reward_net.cnn[3](conv1_activations)\n",
    "pool_activations = reward_net.cnn[4](relu1_activations)\n",
    "flat_activations = reward_net.cnn[5](pool_activations)\n",
    "outputs = reward_net.cnn[-1](flat_activations)\n",
    "\n",
    "print(f\"layer 0 activations:\\n{relu0_activations[0]}\")\n",
    "print(f\"layer 1 activations:\\n{relu1_activations[0]}\")\n",
    "print(f\"flat_activations:\\n{flat_activations[0]}\")\n",
    "print(f\"outputs:\\n{outputs[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight: [-0.10977456 -0.38889107 -0.1206716  -0.38175228 -0.28921402]\n",
      "bias: 0.34487465023994446\n",
      "weight: [ 0.11769152  0.23302093 -0.40303385  0.11292398  0.17897412]\n",
      "bias: 0.3653581142425537\n",
      "\n",
      "\n",
      "weight: [-0.17857413 -0.36234197]\n",
      "bias: -0.5028236508369446\n",
      "weight: [0.5715578  0.04654616]\n",
      "bias: -0.3627457618713379\n",
      "weight: [-0.45337042  0.31298742]\n",
      "bias: 0.548060417175293\n",
      "weight: [-0.06371312 -0.561359  ]\n",
      "bias: -0.6052233576774597\n",
      "weight: [ 0.08518305 -0.09132026]\n",
      "bias: -0.05851120129227638\n"
     ]
    }
   ],
   "source": [
    "# Trying to inspect weights of reward net\n",
    "\n",
    "for weight, bias in zip(reward_net.cnn[0].weight, reward_net.cnn[0].bias):\n",
    "    print(f\"weight: {weight.detach().numpy().flatten()}\")\n",
    "    print(f\"bias: {bias.detach().numpy()}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# for weight, bias in zip(reward_net.cnn[2].weight, reward_net.cnn[2].bias):\n",
    "#     print(f\"weight: {weight.detach().numpy().flatten()}\")\n",
    "#     print(f\"bias: {bias.detach().numpy()}\")\n",
    "\n",
    "print()\n",
    "\n",
    "for weight, bias in zip(reward_net.cnn[-1].weight, reward_net.cnn[-1].bias):\n",
    "    print(f\"weight: {weight.detach().numpy().flatten()}\")\n",
    "    print(f\"bias: {bias.detach().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to write down optimal weights for reward net\n",
    "\n",
    "optimal_conv0_weights = np.array([\n",
    "    [1, 0, 0, 1, 0.1],  # Is agent at home with pellets?\n",
    "    [1, 0, 1, 0, 0],  # Is agent at owned pellet?\n",
    "]).reshape(reward_net.cnn[0].weight.shape)\n",
    "optimal_conv0_bias = np.array([-2, -1]).reshape(reward_net.cnn[0].bias.shape)\n",
    "# optimal_conv1_weights = np.array([\n",
    "#     [0, 1],  # Is agent at owned pellet?\n",
    "#     [10, 0],  # Num pellets to deposit\n",
    "# ]).reshape(reward_net.cnn[2].weight.shape)\n",
    "# optimal_conv1_bias = np.array([0, 0]).reshape(reward_net.cnn[2].bias.shape)\n",
    "optimal_fc_weights = np.array([\n",
    "    [0, 0],  # UP\n",
    "    [0, 0],  # DOWN\n",
    "    [0, 0],  # LEFT\n",
    "    [0, 0],  # RIGHT\n",
    "    [10, -2],  # INTERACT\n",
    "]).reshape(reward_net.cnn[-1].weight.shape)\n",
    "optimal_fc_bias = np.array([0, 0, 0, 0, 0]).reshape(reward_net.cnn[-1].bias.shape)\n",
    "\n",
    "def set_weights(cnn):\n",
    "    cnn[0].weight.data = th.Tensor(optimal_conv0_weights)\n",
    "    cnn[0].bias.data = th.Tensor(optimal_conv0_bias)\n",
    "    # cnn[2].weight.data = th.Tensor(optimal_conv1_weights)\n",
    "    # cnn[2].bias.data = th.Tensor(optimal_conv1_bias)\n",
    "    cnn[-1].weight.data = th.Tensor(optimal_fc_weights)\n",
    "    cnn[-1].bias.data = th.Tensor(optimal_fc_bias)\n",
    "\n",
    "set_weights(reward_net.cnn)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assisting_bounded_humans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
